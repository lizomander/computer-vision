{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIMcHlT48iz1"
      },
      "source": [
        "# Preparation for Milestone Four\n",
        "\n",
        "Today, we will begin preparing for the final milestone. Here, we will assemble all the pieces of the pipeline you've created. You will need to write a function **compute_AgNOR_score**. This function first utilizes the detection model to locate cells within a given image and then feeds those cells into a classification model to classify them into one of the AgNOR classes. Finally, you will aggregate all predictions into a final AgNOR score for the entire image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkIk62AnvEmt",
        "outputId": "70989f23-9ad6-465a-fced-5712ee6bf01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.7)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.4)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchmetrics albumentations opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WjSR9W4uvGqq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torchvision\n",
        "from torchvision.models.detection import RetinaNet\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead, AnchorGenerator\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmSPZEpdvIr-",
        "outputId": "9afce7a0-ec19-4b66-fcc8-4ae4afc1b843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "          filename  max_x  max_y  min_x  min_y  label\n",
            "0  AgNOR_0495.tiff     26     41      4     15      1\n",
            "1  AgNOR_0495.tiff     71     23     42      0      2\n",
            "2  AgNOR_0495.tiff    133     61    104     37      1\n",
            "3  AgNOR_0495.tiff    143    117    121     88      2\n",
            "4  AgNOR_0495.tiff    224     37    199     12      1\n",
            "Index(['filename', 'max_x', 'max_y', 'min_x', 'min_y', 'label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# path to the link you created\n",
        "annotations_path = '/content/gdrive/MyDrive/AgNORs/annotation_frame.p'\n",
        "path_to_slides = '/content/gdrive/MyDrive/AgNORs/'\n",
        "\n",
        "# mount the data\n",
        "drive.mount('/content/gdrive')\n",
        "annotations = pd.read_pickle(annotations_path)\n",
        "print(annotations.head())\n",
        "print(annotations.columns)\n",
        "df = pd.read_csv(path_to_slides +\"annotation_frame.csv\")\n",
        "unique_filenames = df['filename'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1bz59AivoSbE"
      },
      "outputs": [],
      "source": [
        "total_images = len(unique_filenames)\n",
        "train_size = int(total_images * 0.8)\n",
        "test_size = int(total_images * 0.1)\n",
        "validation_size = total_images - train_size - test_size\n",
        "\n",
        "train_images = unique_filenames[:train_size]\n",
        "test_images = unique_filenames[train_size:train_size + test_size]\n",
        "validation_images = unique_filenames[train_size + test_size:]\n",
        "\n",
        "train_df = df[df['filename'].isin(train_images)]\n",
        "val_df = df[df['filename'].isin(validation_images)]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        boxes = self.dataframe.iloc[idx, 1:5].values\n",
        "        boxes = boxes.astype(np.float32).reshape(-1, 4)\n",
        "        target = {'boxes': torch.tensor(boxes), 'labels': torch.tensor(self.dataframe.iloc[idx, 5], dtype=torch.int64).unsqueeze(0)}\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.RandomCrop(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.CenterCrop(224, 224),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_df, path_to_slides, transform=train_transform)\n",
        "val_dataset = CustomDataset(val_df, path_to_slides, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E4Vdbqe8iz5"
      },
      "source": [
        "# 1. Write a function \"process_image\" which receives an image and runs the detection model on it.\n",
        "\n",
        "The function should have the following parameters:\n",
        "\n",
        "1. image: The image on which you want to run inference.\n",
        "2. crop_size: The size of the crops you want to load from the image.\n",
        "3. overlap: Percentage or number of pixels the crops should overlap.\n",
        "4. model: The object detection model. This function should generally be able to run with any detection model.\n",
        "5. detection_threshold: A threshold to apply to the detections to reject false positives.\n",
        "\n",
        "The function will have to tile the image into **overlapping crops** and then feed each crop to the model. After that, all detections have to be transformed to the global coordinate system of the image since the detections are within the coordinate system of the image crop. Subsequently, [non-maximal suppression](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html) needs to be applied to the detections in order to reject overlapping detections. In the end, the function will return the coordinates and scores of the detected cells that exceed the given threshold. Use **torch_no_grad** to save computation time and also ensure your **model is in evaluation mode** before feeding the cells to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e_69PRrmW57k"
      },
      "outputs": [],
      "source": [
        "def process_image(image, crop_size, overlap, model, detection_threshold):\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "    img_h, img_w = image.shape[:2]\n",
        "    if overlap < 1:\n",
        "        step_size = int(crop_size * (1 - overlap))\n",
        "    else:\n",
        "        step_size = crop_size - overlap\n",
        "    model.eval()\n",
        "    detections = []\n",
        "    with torch.no_grad():\n",
        "        for y in range(0, img_h, step_size):\n",
        "            for x in range(0, img_w, step_size):\n",
        "                crop = image[y:y+crop_size, x:x+crop_size]\n",
        "                if crop.shape[0] < crop_size or crop.shape[1] < crop_size:\n",
        "                    pad_h = crop_size - crop.shape[0]\n",
        "                    pad_w = crop_size - crop.shape[1]\n",
        "                    crop = np.pad(crop, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant', constant_values=0)\n",
        "                crop_tensor = torch.tensor(crop).permute(2, 0, 1).unsqueeze(0).float()\n",
        "                output = model(crop_tensor)[0]\n",
        "                for box, score in zip(output['boxes'], output['scores']):\n",
        "                    if score >= detection_threshold:\n",
        "                        global_box = box + torch.tensor([x, y, x, y])\n",
        "                        detections.append((global_box, score))\n",
        "    if len(detections) == 0:\n",
        "        return [], []\n",
        "    boxes, scores = zip(*detections)\n",
        "    boxes = torch.stack(boxes)\n",
        "    scores = torch.tensor(scores)\n",
        "    keep = torchvision.ops.nms(boxes, scores, iou_threshold=0.5)\n",
        "    final_boxes = boxes[keep].tolist()\n",
        "    final_scores = scores[keep].tolist()\n",
        "    return final_boxes, final_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4j6kkkA8iz5"
      },
      "source": [
        "# 2. Write a function \"process_cells\" which classifies the cells from the coordinates that were given to the model.\n",
        "\n",
        "The function should have the following parameters:\n",
        "\n",
        "1. image: The image from which to load the cells.\n",
        "2. coords: Coordinates of the cells which you found with the detection algorithm.\n",
        "3. model: The trained classification model.\n",
        "4. crop_size: A size to resize the crops to. It should be equal to the size with which you trained the classification network.\n",
        "\n",
        "The function should load each cell from the respective image and feed them to the classification model. Save the prediction and, in the end, aggregate the classifications of all cells into a final AgNOR score. The function should return the labels of the respective cells as well as the final AgNOR score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nOhGkcFtXGy2"
      },
      "outputs": [],
      "source": [
        "def process_cells(image, coords, model, crop_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((crop_size, crop_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    model.eval()\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for box in coords:\n",
        "            cropped_image = image.crop(box)\n",
        "            input_tensor = transform(cropped_image).unsqueeze(0)\n",
        "            output = model(input_tensor)\n",
        "            label = output.argmax(1).item()\n",
        "            labels.append(label)\n",
        "    AgNOR_score = sum(labels) / len(labels) if labels else 0\n",
        "    return labels, AgNOR_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8-WaGJq8iz6"
      },
      "source": [
        "# 3. Combine both functions into the function **compute_AgNOR_score**.\n",
        "\n",
        "This function should receive the image as a parameter and also require all parameters to execute the subfunctions. In the end, this function should return the overall AgNOR score of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "F6buEeZxXL2p"
      },
      "outputs": [],
      "source": [
        "def compute_AgNOR_score(image, detection_model, classification_model, crop_size, overlap, detection_threshold):\n",
        "    boxes, scores = process_image(image, crop_size, overlap, detection_model, detection_threshold)\n",
        "    labels, AgNOR_score = process_cells(image, boxes, classification_model, crop_size)\n",
        "    return AgNOR_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DohP4pei8iz6"
      },
      "source": [
        "# 4. Test your pipeline.\n",
        "\n",
        "Take several images (approximately 5) and run them through your pipeline. Then, calculate the error between the predicted AgNOR score and the AgNOR score defined by the labels of the cells in the annotation file. To obtain this label, simply calculate the mean of the labels of the respective image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nzavmvMFq2CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15295cff-5a38-45e8-95f6-73294fe5e639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 1.9812982357580686\n"
          ]
        }
      ],
      "source": [
        "sample_images = unique_filenames[:5]\n",
        "ground_truth_scores = []\n",
        "for image_name in sample_images:\n",
        "    image_annotations = df[df['filename'] == image_name]\n",
        "    ground_truth_score = image_annotations['label'].mean()\n",
        "    ground_truth_scores.append(ground_truth_score)\n",
        "\n",
        "detection_model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "classification_model = torchvision.models.resnet50(pretrained=True)\n",
        "classification_model.fc = torch.nn.Linear(classification_model.fc.in_features, 2)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "classification_model.to(device)\n",
        "classification_model.eval()\n",
        "\n",
        "crop_size = 224\n",
        "overlap = 0.2\n",
        "detection_threshold = 0.5\n",
        "\n",
        "predicted_scores = []\n",
        "for image_name in sample_images:\n",
        "    image_path = os.path.join(path_to_slides, image_name)\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    predicted_score = compute_AgNOR_score(image, detection_model, classification_model, crop_size, overlap, detection_threshold)\n",
        "    predicted_scores.append(predicted_score)\n",
        "\n",
        "errors = [abs(pred - gt) for pred, gt in zip(predicted_scores, ground_truth_scores)]\n",
        "mean_error = sum(errors) / len(errors)\n",
        "print(f'Mean Absolute Error: {mean_error}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}